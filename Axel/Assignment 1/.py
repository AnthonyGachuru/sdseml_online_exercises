{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. n_estimators are the trees you build in the decision tree. The higher number of n_estimators enusres the perfomance of the code and give stronger predictions.-\n",
    "# However, the n_estimators include diminishing returns and thus comes with a computational risk you should therefor tune as high af value as the processor can handle.\n",
    "#2. max_depth is the maximum depth of the tree. The deeper the tree the more complex the decisions get, and could result in overfitting the model making it useless in real applications.\n",
    "#If there max_depth is set to \"None\" then nodes are expanded until all leaves are pure or until the all leaves contain less than the \n",
    "#minimum_sample_split samples which is the minimum number of samples required to split an internal node.\n",
    "#3. max_features is the number of features to consider when looking for the best split. The max_features is set up as an if/then construction.\n",
    "#By specifying enough max_features you will obtain a better chance of finding the best split. Max_features can however increase the correlation and thus increase varitaion of the model.\n",
    "#4. Bootstrap is a boolean varible (True/False) and is used to pick out the dataset. If False, the whole dataset is used to build each tree. \n",
    "\n",
    "##2\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Wine data\n",
    "data_wine = load_wine()\n",
    "X_wine = data_wine['data']\n",
    "y_wine = data_wine['target']\n",
    "\n",
    "# Digits data\n",
    "data_digits = load_digits()\n",
    "X_digits = data_digits['data']\n",
    "y_digits = data_digits['target']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#For digits\n",
    "\n",
    "PCAx = PCA().fit_transform(X_digits,y_digits)\n",
    "    \n",
    "LDAx = LDA().fit_transform(X_digits,y_digits)\n",
    "    \n",
    "tSNEx = TSNE().fit_transform(X_digits,y_digits)\n",
    "    \n",
    "umapx = UMAP().fit_transform(X_digits,y_digits)\n",
    "\n",
    "#Plot for digits\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.suptitle(\"Digits\")\n",
    "plt.subplot(2,2,1)\n",
    "plt.scatter(PCAx[:,0], PCAx[:,1], c=y_digits, cmap='viridis')\n",
    "plt.title(\"Principal Component Analysis\", fontsize=10)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(LDAx[:,0], LDAx[:,1], c=y_digits, cmap='viridis')\n",
    "plt.title(\"Linear Discriminant Analysis\", fontsize=10)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(tSNEx[:,0], tSNEx[:,1], c=y_digits, cmap='viridis')\n",
    "plt.title(\"t-Distributed Stochastic Neighbor Embedding\", fontsize=10)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.scatter(umapx[:,0], umapx[:,1], c=y_digits, cmap='viridis')\n",
    "plt.title(\"Uniform Manifold Approximation and Projections\", fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#For wine\n",
    "\n",
    "# Standardize features by removing mean and scale to unit variance. Features will center at origin.\n",
    "sc = StandardScaler()\n",
    "X_std_wine = sc.fit_transform(X_wine)\n",
    "\n",
    "# create covariance matrix. Could also use correlation matrix\n",
    "cov_mat = np.cov(X_std_wine.T)\n",
    "\n",
    "# create eigenvectors and eigen values to find PC1 and PC2:\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "# Sort the eigenvalues by decreasing order to rank eigenvectors \n",
    "eigen_vals, eigen_vecs = zip(*sorted(\n",
    "    zip(abs(eigen_vals), eigen_vecs.T),\n",
    "    key=lambda kv: kv[0], reverse=True\n",
    "))\n",
    "\n",
    "# Get the top 2 eigenvectors\n",
    "top_k_eigen_vecs = eigen_vecs[:2]\n",
    "\n",
    "# Compose basis transformation matrix\n",
    "W = np.hstack([\n",
    "    w.reshape(-1, 1) for w in top_k_eigen_vecs\n",
    "])\n",
    "\n",
    "# Transform datapoints\n",
    "Z = np.dot(X_std_wine, W)\n",
    "\n",
    "PCAw = PCA().fit_transform(Z)\n",
    "    \n",
    "LDAw = LDA().fit_transform(Z, y_wine)\n",
    "    \n",
    "tSNEw = TSNE().fit_transform(Z)\n",
    "    \n",
    "umapw = UMAP().fit_transform(Z)\n",
    "\n",
    "#Plot for wine using the standardized values\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.suptitle(\"Wine\")\n",
    "plt.subplot(2,2,1)\n",
    "plt.scatter(PCAw[:,0], PCAw[:,1], c=y_wine, cmap='viridis')\n",
    "plt.title(\"Principal Component Analysis\", fontsize=10)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(LDAw[:,0], LDAw[:,1], c=y_wine, cmap='viridis')\n",
    "plt.title(\"Linear Discriminant Analysis\", fontsize=10)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(tSNEw[:,0], tSNEw[:,1], c=y_wine, cmap='viridis')\n",
    "plt.title(\"t-Distributed Stochastic Neighbor Embedding\", fontsize=10)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.scatter(umapw[:,0], umapw[:,1], c=y_wine, cmap='viridis')\n",
    "plt.title(\"Uniform Manifold Approximation and Projections\", fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}